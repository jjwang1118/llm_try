[global]
do_train = true
do_eval = false
seo = targeted


[train.basic]
model = model/Mistral-7B-v0.1
tokenizer = model/Mistral-7B-v0.1
train_data = data/126_csdn/126_csdn_train.json
validation_data = 

[train.trainer]
prompmt_template_id = 0
precistion = half


[train.lora]
r = 16
target_modules = ['q_proj','k_proj','v_proj']
lora_alpha = 32
lora_dropout = 0.2
bias = none 
init_lora_weights = true 

[train.hyperparameters]
output_dir = checkpoints/mistral_7b_126_csdn/
per_device_train_batch_size = 4
gradient_accumulation_steps = 64
weight_decay = 0.01
learning_rate = 5e-4
num_train_epochs = 3
warmup_ratio = 0.1
logging_steps = 1
save_steps = 200
data_seed = 42
seed = 42
bf16 = true
evaluation_strategy= steps
eval_steps = 100
per_device_eval_batch_size = 16
eval_delay = 1
optim = adamw_torch
label_smoothing_factor = 0